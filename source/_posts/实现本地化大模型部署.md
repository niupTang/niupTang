---
title: 实现本地化大模型部署
date: '2026-01-04T16:26:13.968285'
updated: '2026-01-05T23:18:08.854652'
tags: []
categories: []
cover: null
---

Ollama 是一个开源框架，专为在本地机器上便捷部署和运行大型语言模型（LLM）而设计�?

     可以通过Ollama官网直接下载，下载完成后直接通过命令行安装大模型，通过命令安装大模型速度有点慢，因为大模型的包比较大�?

      如今 LLM 早已不再等同于昂贵的 GPU，而是可以在大部分消费级计算机上运行推理的应用了——俗称本地大模型�?

      根据经验�?6 位浮点精度（FP16）的模型，推理所需显存（以 GB 为单位）约    为模型参数量（�?10 亿为单位）的两倍。据此，Llama 2 7B�?0 亿）对应需要约 14GB 显存以进行推理，这显然超出了普通家用计算机的硬件规格。作为参考，一�?GeForce RTX 4060 Ti 16GB 显卡市场价超�?3000 元�?

      模型量化（quantization）技术可以很大程度上降低显存要求。以 4-bit 量化为例，其将原�?FP16 精度的权重参数压缩为 4 位整数精度，使模型权重体积和推理所需显存均大幅减小，仅需 FP16 �?1/4 �?1/3，意味着�?4GB 显存即可启动 7B 模型的推理（当然实际显存需求会随着上下文内容叠加而不断增大）�?

**安装Ollama**

下载地址：https://ollama.com/download/windows

   

![](https://mmbiz.qpic.cn/mmbiz_png/p1ESIQQvfrR2rj8IbEIetA9xlicluGbAQIvzLGia4BSMnzRKkRjicNA6BeicuP8UYnN9C1vVm2m4yNw7w4xcon0hbw/640?wx_fmt=png&from=appmsg)

下载版本存在windows和macOS 还有Linux,.

在window系统下载好了过后可以通过傻瓜式的安装�?

完成后通过 通过命令直接安装Ollama上面支持的模型�?

![](https://mmbiz.qpic.cn/mmbiz_png/p1ESIQQvfrR2rj8IbEIetA9xlicluGbAQXJfiahwKvo3tDMpusr4EAIiccOibgPCDdmxwOrtA0ask39oEpSvkb5lYg/640?wx_fmt=png&from=appmsg)

通过命令运行 ollama run llama2-chinese 
![](https://mmbiz.qpic.cn/mmbiz_png/p1ESIQQvfrRWhB2bzKsq7ANxhdRVp79DyA40fE3phfHdsnGjCdjt9kKHG6U6ALqicrLQJCEa7vkgnf2LyVLtWLQ/640?wx_fmt=png&from=appmsg)

提问题，发现该模型给了很好的回复�?
![](https://mmbiz.qpic.cn/mmbiz_png/p1ESIQQvfrRWhB2bzKsq7ANxhdRVp79D9AwKxeT7hJmAqCyzmlqqEbicxWo2LNk2vfcFN0PkNG4XRkx8N4HKu1A/640?wx_fmt=png&from=appmsg)

 

   这样一个本地的大模型就安装完成了，再结合之前说的fastGPT 就可以整合本地资源对大模型上传文件了�